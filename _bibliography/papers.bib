@article{ma2024licrocc,
  title={LiCROcc: Teach radar for accurate semantic occupancy prediction using lidar and camera},
  author={Ma, Yukai and Mei, Jianbiao and Yang, Xuemeng and Wen, Licheng and Xu, Weihua and Zhang, Jiangning and Zuo, Xingxing and Shi, Botian and Liu, Yong},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
  abbr={RAL},
  publisher={IEEE},
  code        = {https://github.com/HR-zju/LiCROcc},
  demo        = {https://hr-zju.github.io/LiCROcc/},
  selected     = {true},
  preview     = {licrocc.png},
  bibtex_show = {true},

}



@article{yang2024drivearena,
  title={DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving}, 
  author={Xuemeng Yang* and Licheng Wen* and Yukai Ma* and Jianbiao Mei* and Xin Li* and Tiantian Wei* and Wenjie Lei and Daocheng Fu and Pinlong Cai and Min Dou and Botian Shi and Liang He and Yong Liu and Yu Qiao},
  journal={arXiv preprint arXiv:2408.00415},
  year={2024},
  abstract    = {This paper presetns DriveArena, the first high-fidelity closed-loop simulation system designed for driving agents navigating in real scenarios. DriveArena features a flexible, modular architecture, allowing for the seamless interchange of its core components: Traffic Manager, a traffic simulator capable of generating realistic traf- fic flow on any worldwide street map, and World Dreamer, a high-fidelity conditional generative model with infinite autoregression. This powerful synergy empowers any driving agent capable of processing real-world images to navigate in DriveArena simulated environment. The agent perceives its surroundings through images generated by World Dreamer and output trajectories; then these trajectories are fed into Traffic Manager, achieving realistic interactions with other vehicles and producing a new scene lay- out. Finally, the latest scene layout is relayed back into World Dreamer, perpetuating the simulation cycle. This iterative process fosters closed-loop exploration within a highly realistic environment, providing a valuable platform for developing and evaluating driving agents across diverse and challenging scenarios. DriveArena signifies a substantial leap forward in leveraging generative image data for the driving simulatior, opening insights for closed-loop autonomous driving.},
  abbr        = {Preprint},
  arxiv       = {2408.00415},
  bibtex_show = {true},
  preview     = {drivearena.png},
  code        = {https://github.com/pjlab-adg/DriveArena},
  demo        = {https://pjlab-adg.github.io/DriveArena/},
  selected     = {true},

}

@article{mei2024continuously,
  title       = {Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving},
  author      = {Mei*, Jianbiao and Ma*, Yukai and Yang, Xuemeng and Wen, Licheng and Cai, Xinyu and Li, Xin and Fu, Daocheng and Zhang, Bo and Cai, Pinlong and Dou, Min and others},
  journal     = {Advances in Neural Information Processing Systems (NeurIPS)},
  year        = {2024},
  abstract    = {Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement.},
  abbr        = {NeurIPS},
  arxiv       = {2405.15324},
  bibtex_show = {true},
  preview     = {leapad.png},
  code        = {https://github.com/pjlab-adg/leapad},
  demo        = {https://leapad-2024.github.io/LeapAD/},
  selected     = {true},
}

@ARTICLE{9869309,
  author={Lang, Xiaolei and Lv, Jiajun and Huang, Jianxin and Ma, Yukai and Liu, Yong and Zuo, Xingxing},
  journal={IEEE Robotics and Automation Letters}, 
  title={Ctrl-VIO: Continuous-Time Visual-Inertial Odometry for Rolling Shutter Cameras}, 
  year={2022},
  volume={7},
  number={4},
  pages={11537-11544},
  keywords={Cameras;Delays;Calibration;Splines (mathematics);Trajectory;Visualization;Robot vision systems;Localization;sensor fusion;visual-inertial SLAM},
  doi={10.1109/LRA.2022.3202349},
  bibtex_show = {true},
  }

@ARTICLE{10251629,
  author={Lang, Xiaolei and Chen, Chao and Tang, Kai and Ma, Yukai and Lv, Jiajun and Liu, Yong and Zuo, Xingxing},
  journal={IEEE Robotics and Automation Letters}, 
  title={Coco-LIC: Continuous-Time Tightly-Coupled LiDAR-Inertial-Camera Odometry Using Non-Uniform B-Spline}, 
  year={2023},
  volume={8},
  number={11},
  pages={7074-7081},
  keywords={Trajectory;Splines (mathematics);Laser radar;Visualization;Odometry;Cameras;Sensors;LiDAR-inertial-camera SLAM;localization;sensor fusion;state estimation},
  doi={10.1109/LRA.2023.3315542},
  paper={https://ieeexplore.ieee.org/abstract/document/10251629},
  bibtex_show = {true},
  }

  @ARTICLE{10694710,
  author={Mei, Jianbiao and Yang, Yu and Wang, Mengmeng and Zhu, Junyu and Ra, Jongwon and Ma, Yukai and Li, Laijian and Liu, Yong},
  journal={IEEE Transactions on Image Processing}, 
  title={Camera-Based 3D Semantic Scene Completion With Sparse Guidance Network}, 
  year={2024},
  volume={33},
  number={},
  pages={5468-5481},
  keywords={Three-dimensional displays;Semantics;Geometry;Feature extraction;Solid modeling;Proposals;Cameras;Convergence;Autonomous vehicles;Visualization;Semantic scene completion;sparse guidance network;hybrid guidance;voxel aggregation},
  doi={10.1109/TIP.2024.3461989}}

@INPROCEEDINGS{10610929,
  author={Li*, Han and Ma*, Yukai and Gu, Yaqing and Hu, Kewei and Liu, Yong and Zuo, Xingxing},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale}, 
  year={2024},
  volume={},
  number={},
  pages={10665-10672},
  keywords={Point cloud compression;Image coding;Accuracy;Three-dimensional displays;Robot vision systems;Estimation;Radar},
  doi={10.1109/ICRA57147.2024.10610929}}


@ARTICLE{10168166,
  author={Li*, Laijian and Ma*, Yukai and Tang, Kai and Zhao, Xiangrui and Chen, Chao and Huang, Jianxin and Mei, Jianbiao and Liu, Yong},
  journal={IEEE Robotics and Automation Letters}, 
  title={Geo-Localization With Transformer-Based 2D-3D Match Network}, 
  year={2023},
  volume={8},
  number={8},
  pages={4855-4862},
  keywords={Laser radar;Point cloud compression;Feature extraction;Three-dimensional displays;Satellites;Location awareness;Global Positioning System;Geo-localization;2D-3D match;SLAM},
  doi={10.1109/LRA.2023.3290526},
  bibtex_show = {true},
  code={https://github.com/yzdad/D-GLSNet},
  selected     = {true},
  abbr={RAL},

  }




@article{fu2023overlapnetvlad,
  title={OverlapNetVLAD: A coarse-to-fine framework for LiDAR-based place recognition},
  author={Fu, Chencan and Li, Lin and Peng, Linpeng and Ma, Yukai and Zhao, Xiangrui and Liu, Yong},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}

@article{chen2023ol,
  title={OL-SLAM: a robust and versatile system of object localization and slam},
  author={Chen*, Chao and Ma*, Yukai and Lv, Jiajun and Zhao, Xiangrui and Li, Laijian and Liu, Yong and Gao, Wang},
  journal={Sensors},
  volume={23},
  number={2},
  pages={801},
  year={2023},
  publisher={MDPI}
}

@ARTICLE{10623522,
  author={Li*, Han and Ma*, Yukai and Huang, Yuehao and Gu, Yaqing and Xu, Weihua and Liu, Yong and Zuo, Xingxing},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={RIDERS: Radar-Infrared Depth Estimation for Robust Sensing}, 
  year={2024},
  volume={25},
  number={11},
  pages={18764-18778},
  keywords={Radar;Radar imaging;Estimation;Cameras;Measurement;Laser radar;Accuracy;Autonomous driving;Infrared imaging;Multisensor systems;Depth estimation;radar perception;infrared camera;multi-sensor fusion},
  doi={10.1109/TITS.2024.3432996},
    bibtex_show = {true},
  code={https://github.com/MMOCKING/RIDERS},
  selected     = {true},
  abbr={TITS},
  preview={riders.png}
  }

  @INPROCEEDINGS{10161203,
  author={Ma, Yukai and Zhao, Xiangrui and Li, Han and Gu, Yaqing and Lang, Xiaolei and Liu, Yong},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RoLM: Radar on LiDAR Map Localization}, 
  year={2023},
  volume={},
  number={},
  pages={3976-3982},
  keywords={Location awareness;Laser radar;Automation;Autonomous systems;Robot sensing systems;Cameras;Robustness},
  doi={10.1109/ICRA48891.2023.10161203},
  selected  = {true},
  bibtex_show = {true},
  preview={rolm.png},
  abbr={ICRA}}


@inproceedings{yang2025driving,
  title={Driving in the occupancy world: Vision-centric 4d occupancy forecasting and planning via world models for autonomous driving},
  author={Yang, Yu and Mei, Jianbiao and Ma, Yukai and Du, Siliang and Chen, Wenqing and Qian, Yijie and Feng, Yuxiang and Liu, Yong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={9},
  pages={9327--9335},
  year={2025},
  abbr={AAAI}},

}

@INPROCEEDINGS{10802605,
  author={Tang, Kai and Lang, Xiaolei and Ma, Yukai and Huang, Yuehao and Li, Laijian and Liu, Yong and Lv, Jiajun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Monocular Event-Inertial Odometry with Adaptive decay-based Time Surface and Polarity-aware Tracking}, 
  year={2024},
  volume={},
  number={},
  pages={12544-12551},
  keywords={Power demand;Tracking;Dynamics;Cameras;Feature extraction;Robustness;Surface texture;High dynamic range;Odometry;Intelligent robots},
  doi={10.1109/IROS58592.2024.10802605},
    abbr={IROS}},
}

@article{ma2024fmcw,
  title={FMCW Radar on LiDAR map localization in structural urban environments},
  author={Ma*, Yukai and Li*, Han and Zhao, Xiangrui and Gu, Yaqing and Lang, Xiaolei and Li, Laijian and Liu, Yong},
  journal={Journal of Field Robotics},
  volume={41},
  number={3},
  pages={699--717},
  year={2024},
  selected= {true},
  publisher={Wiley Online Library},
  abbr={JFR},
  code={https://github.com/HR-zju/ZJU-Radar-Dataset},
}

@article{ma2025leapvad,
  title={LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking},
  author={Ma, Yukai and Wei, Tiantian and Zhong, Naiting and Mei, Jianbiao and Hu, Tao and Wen, Licheng and Yang, Xuemeng and Shi, Botian and Liu, Yong},
  journal={arXiv preprint arXiv:2501.08168},
  year={2025},
  abbr={Arxiv},
  demo={https://pjlab-adg.github.io/LeapVAD/},
  code={https://github.com/PJLab-ADG/LeapVAD},
  selected={true},
  preview={leapvad.png}

}

@inproceedings{fu2024coarse,
  title={A Coarse-to-Fine Place Recognition Approach using Attention-guided Descriptors and Overlap Estimation},
  author={Fu, Chencan and Li, Lin and Mei, Jianbiao and Ma, Yukai and Peng, Linpeng and Zhao, Xiangrui and Liu, Yong},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={8493--8499},
  year={2024},
  organization={IEEE}
}

@article{xiang2023subp,
  title={SUBP: Soft Uniform Block Pruning for 1$$\backslash$times $ N Sparse CNNs Multithreading Acceleration},
  author={Xiang, Jingyang and Li, Siqi and Chen, Jun and Dai, Guang and Bai, Shipeng and Ma, Yukai and Liu, Yong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52033--52050},
  year={2023}
}

@article{wang2025l2cocc,
  title={L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model},
  author={Wang, Ruoyu and Ma, Yukai and Yao, Yi and Tao, Sheng and Li, Haoang and Zhu, Zongzhi and Liu, Yong and Zuo, Xingxing},
  journal={arXiv preprint arXiv:2503.12369},
  year={2025}
}